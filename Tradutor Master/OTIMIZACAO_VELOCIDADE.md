# âš¡ OtimizaÃ§Ã£o de Velocidade - Batches Gigantes

## ğŸ¯ EstratÃ©gia Implementada

Baseado no seu insight: **"se no chat podemos colocar o documento inteiro e ele traduz em segundos, por que nÃ£o traduzir vÃ¡rios segmentos numa Ãºnica requisiÃ§Ã£o?"**

### MudanÃ§a de EstratÃ©gia

**ANTES** (Paralelo com batches pequenos):
```
ğŸ“„ Documento com 1500 segmentos
   â†“
ğŸ”€ Dividido em 10 batches de 150 segmentos
   â†“
âš¡ 10 workers processando em paralelo
   â†“
âŒ PROBLEMA: 10 requisiÃ§Ãµes simultÃ¢neas = Rate Limit (429 errors)
   â†“
â± Retry de 6-7 segundos a cada erro = MUITO LENTO
```

**DEPOIS** (Serial com batches gigantes):
```
ğŸ“„ Documento com 1500 segmentos
   â†“
ğŸ“¦ 1 ÃšNICO batch gigante de 1500 segmentos
   â†“
ğŸ¯ 1 worker fazendo 1 requisiÃ§Ã£o
   â†“
âœ… RESULTADO: 1 requisiÃ§Ã£o = SEM rate limit
   â†“
âš¡ TraduÃ§Ã£o em ~10-20 segundos = RÃPIDO
```

## ğŸ“Š ConfiguraÃ§Ãµes Implementadas

### Batch Sizes por Modelo

```python
OPTIMAL_BATCH_SIZES = {
    "claude-3-5-haiku-20241022": 1500,   # ğŸš€ GIGANTE (200K contexto)
    "claude-3-5-sonnet-20241022": 800,   # Grande (200K contexto)
    "claude-3-haiku-20240307": 1000,     # Grande (200K contexto)
    "claude-3-sonnet-20240229": 500,     # MÃ©dio
    "claude-3-opus-20240229": 400,       # MÃ©dio
}
```

### Workers

```python
max_workers = 1  # Apenas 1 worker = SEM paralelismo = SEM rate limit
```

### Delay Entre RequisiÃ§Ãµes

```python
# SEM delay artificial
# O tempo de processamento do batch gigante (~10-20s)
# jÃ¡ garante que ficamos abaixo do rate limit (50 RPM)
```

## ğŸ§® CÃ¡lculo dos Batch Sizes

### Base do CÃ¡lculo

**Haiku 3.5**:
- Contexto: 200,000 tokens
- Segment mÃ©dio: ~50 tokens (texto + JSON)
- Capacidade teÃ³rica: 200,000 Ã· 50 = **4,000 segmentos**
- Conservador (30% do contexto): **1,200 segmentos**
- **Implementado: 1,500 segmentos** (meio termo seguro)

### Por Que 1500?

1. **Seguro**: Fica abaixo do limite real de contexto
2. **Eficiente**: A maioria dos documentos cabe em 1-2 requisiÃ§Ãµes
3. **Sem Rate Limit**: 1 requisiÃ§Ã£o por vez = mÃ¡ximo 1 RPM (limite Ã© 50 RPM)

## ğŸ“ˆ ComparaÃ§Ã£o de Performance

### Documento Pequeno (500 segmentos)

| EstratÃ©gia | RequisiÃ§Ãµes | Rate Limit | Tempo |
|-----------|-------------|------------|-------|
| **ANTES** | 3-5 paralelas | âŒ Sim (429) | ~30-40s |
| **DEPOIS** | 1 serial | âœ… NÃ£o | ~10-15s |

### Documento MÃ©dio (1500 segmentos)

| EstratÃ©gia | RequisiÃ§Ãµes | Rate Limit | Tempo |
|-----------|-------------|------------|-------|
| **ANTES** | 10-15 paralelas | âŒâŒ Muito (429) | ~60-90s |
| **DEPOIS** | 1 serial | âœ… NÃ£o | ~15-20s |

### Documento Grande (3000 segmentos)

| EstratÃ©gia | RequisiÃ§Ãµes | Rate Limit | Tempo |
|-----------|-------------|------------|-------|
| **ANTES** | 20-30 paralelas | âŒâŒâŒ Extremo (429) | ~120-180s |
| **DEPOIS** | 2 seriais | âœ… NÃ£o | ~30-40s |

## âœ… BenefÃ­cios

1. **Sem 429 Errors**: Apenas 1 requisiÃ§Ã£o por vez = impossÃ­vel exceder 50 RPM
2. **Sem Retries**: Sem erros 429 = sem delays de 6-7 segundos
3. **Mais RÃ¡pido**: Menos overhead de requisiÃ§Ãµes HTTP
4. **Aproveitamento MÃ¡ximo**: Usa 200K de contexto eficientemente
5. **Simples**: Sem complexidade de paralelismo

## ğŸ” Como Funciona na PrÃ¡tica

### Exemplo Real

**Arquivo**: `KERRY_PROJECT_LOGISTICS_-_0031620659_000.docx`

**Fluxo**:
```
1. ExtraÃ§Ã£o: 1,234 segmentos
   â†“
2. Batch Ãºnico de 1,234 segmentos
   â†“
3. Claude processa em ~12 segundos
   â†“
4. Pronto! âœ…
```

**Antes**:
- 8-10 requisiÃ§Ãµes paralelas
- 3-4 erros 429
- Retries de 6-7s cada
- Total: ~45-60 segundos

**Depois**:
- 1 requisiÃ§Ã£o
- 0 erros 429
- 0 retries
- Total: ~12 segundos

## ğŸ“ Notas TÃ©cnicas

### Rate Limit Natural

Com batches de 1500 segmentos:
- Tempo de processamento: ~10-20s por requisiÃ§Ã£o
- RequisiÃ§Ãµes por minuto: ~3-6 RPM
- Limite da API: 50 RPM
- **Margem de seguranÃ§a**: 8-16x abaixo do limite!

### Contexto de 200K

O Claude Haiku 3.5 tem 200,000 tokens de contexto:
```
Prompt do sistema:     ~500 tokens
InstruÃ§Ãµes:            ~300 tokens
GlossÃ¡rio (opcional):  ~1,000 tokens
Batch de 1500 segs:    ~75,000 tokens
Resposta (1500 segs):  ~75,000 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                 ~152,000 tokens (76% do limite)
```

âœ… Sobra 48,000 tokens de margem de seguranÃ§a!

## ğŸ¯ Quando Usar Cada EstratÃ©gia

### Batches Gigantes (Atual - Recomendado)
- âœ… Documentos Ãºnicos
- âœ… Batch de mÃºltiplos documentos
- âœ… Quando rate limit Ã© problema
- âœ… Quando quer velocidade mÃ¡xima

### Paralelo (Desativado)
- âŒ NÃ£o recomendado
- âŒ Causa rate limit
- âŒ Mais lento devido a retries

## ğŸ”§ Arquivos Modificados

- **[src/claude_client.py](src/claude_client.py#L53-L63)**: Batch sizes aumentados
- **[src/claude_client.py](src/claude_client.py#L98-L104)**: Workers reduzido para 1
- **[src/claude_client.py](src/claude_client.py#L137-L139)**: Delay removido

---

**ImplementaÃ§Ã£o**: Baseado no insight do usuÃ¡rio sobre aproveitar o contexto de 200K tokens!
