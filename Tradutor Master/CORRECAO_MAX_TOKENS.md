# üö® CORRE√á√ÉO: Limite de max_tokens (Output)

## ‚ùå Problema Identificado

**Erro**: `Unterminated string starting at: line 142 column 41`

**Causa**: Batch de **1800 segmentos** excedeu o limite de `max_tokens` (output).

### An√°lise do JSON Salvo

Arquivo: `claude_json_errors/claude_error_20260104_103817.json`

√öltima linha (cortada):
```json
{"location": "T139", "translation": "- O SUBCONTRATADO declara que est√° totalmente experiente, devidamente qualificado, regist
```

**O que aconteceu**:
1. Claude come√ßou a gerar JSON com 1800 tradu√ß√µes
2. Ap√≥s gerar 140 tradu√ß√µes (~11,900 caracteres)
3. Atingiu o limite de `max_tokens = 8192`
4. Output foi **cortado no meio**, deixando string n√£o terminada
5. JSON inv√°lido ‚Üí Erro de parse

## üìä C√°lculo do Problema

### Output Necess√°rio vs Dispon√≠vel

**Para 1800 segmentos**:
```
Cada linha JSON: ~60 tokens
  {"location": "T123", "translation": "texto aqui..."}

1800 segmentos √ó 60 tokens = 108,000 tokens necess√°rios
```

**Limite dispon√≠vel**:
```
max_tokens (Haiku 3.5) = 8,192 tokens
```

**Resultado**:
```
108,000 tokens necess√°rios
  8,192 tokens dispon√≠veis
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IMPOSS√çVEL! Output cortado ap√≥s ~136 linhas
```

## ‚úÖ Solu√ß√£o Implementada

### Batch Sizes Ajustados

**ANTES** (Baseado no contexto de 200K):
```python
OPTIMAL_BATCH_SIZES = {
    "claude-3-5-haiku-20241022": 2000,  # ‚ùå MUITO GRANDE!
    "claude-3-haiku-20240307": 1800,    # ‚ùå MUITO GRANDE!
    "claude-3-5-sonnet-20241022": 1500, # ‚ùå MUITO GRANDE!
}
```

**AGORA** (Baseado no max_tokens de OUTPUT):
```python
OPTIMAL_BATCH_SIZES = {
    "claude-3-5-haiku-20241022": 100,   # ‚úÖ 100 √ó 60 = 6,000 tokens
    "claude-3-5-sonnet-20241022": 120,  # ‚úÖ 120 √ó 60 = 7,200 tokens
    "claude-3-haiku-20240307": 60,      # ‚úÖ 60 √ó 60 = 3,600 tokens
    "claude-3-opus-20240229": 60,       # ‚úÖ 60 √ó 60 = 3,600 tokens
}
```

### C√°lculo Correto

```python
# F√≥rmula:
batch_size = (max_tokens √ó 0.85) √∑ 60 tokens_por_linha

# Haiku 3.5:
batch_size = (8192 √ó 0.85) √∑ 60 = ~116 ‚Üí Conservador: 100

# Sonnet 3.5:
batch_size = (8192 √ó 0.85) √∑ 60 = ~116 ‚Üí Conservador: 120

# Haiku 3.0 / Opus:
batch_size = (4096 √ó 0.85) √∑ 60 = ~58 ‚Üí Conservador: 60
```

## üìà Impacto na Performance

### Documento com 2507 Segmentos (seu caso)

**ANTES** (batches grandes):
```
Batch 1: 1800 segmentos ‚Üí ‚ùå ERRO (max_tokens excedido)
Batch 2: 707 segmentos  ‚Üí N√£o executado
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: FALHOU
```

**AGORA** (batches corretos):
```
Batch 1: 100 segmentos ‚Üí ‚úÖ OK (~15s)
Batch 2: 100 segmentos ‚Üí ‚úÖ OK (~15s)
Batch 3: 100 segmentos ‚Üí ‚úÖ OK (~15s)
...
Batch 25: 100 segmentos ‚Üí ‚úÖ OK (~15s)
Batch 26: 7 segmentos ‚Üí ‚úÖ OK (~5s)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: ~26 requisi√ß√µes √ó 15s = ~6-7 minutos
```

### Compara√ß√£o

| Documento | Segmentos | Batches (antes) | Batches (agora) | Tempo |
|-----------|-----------|-----------------|-----------------|-------|
| Pequeno | 92 | 1 ‚úÖ | 1 ‚úÖ | ~10s |
| M√©dio | 500 | 1 ‚ùå (erro) | 5 ‚úÖ | ~1.5min |
| Grande | 2507 | 2 ‚ùå (erro) | 26 ‚úÖ | ~6-7min |

## üéØ Por Que Aconteceu?

### Erro de C√°lculo Inicial

**Pensamento Errado**:
> "Claude tem 200K de contexto, ent√£o posso enviar 2000 segmentos!"

**Realidade**:
- ‚úÖ **Input**: 200K contexto (pode receber muitos segmentos)
- ‚ùå **Output**: 8K max_tokens (pode gerar POUCO texto)

### Analogia

Imagine um caminh√£o:
- **Capacidade de carga** (input/contexto): 200 toneladas ‚úÖ
- **Porta de sa√≠da** (output/max_tokens): Porta de 8cm de largura ‚ùå

Voc√™ pode **CARREGAR** 200 toneladas, mas s√≥ pode **DESCARREGAR** pela portinha pequena!

## üìù Coment√°rio no C√≥digo

```python
# Batch size otimizado por modelo (quantos SEGMENTOS por requisi√ß√£o)
# LIMITADO PELO max_tokens (output)!
# C√°lculo: max_tokens √∑ 60 tokens por linha JSON = segmentos m√°ximos
# - Haiku 3.5: 8192 √∑ 60 = ~136 segmentos (conservador: 100)
# - Sonnet 3.5: 8192 √∑ 60 = ~136 segmentos (conservador: 120)
# IMPORTANTE: O limite √© o OUTPUT, n√£o o contexto!
```

## üîß Arquivos Modificados

- **[src/claude_client.py](src/claude_client.py#L53-L66)**: Batch sizes reduzidos para respeitar max_tokens

## ‚úÖ Valida√ß√£o

### Teste com 92 Segmentos (RENCOTEK)
```
‚úÖ PASSOU: 92 < 100, cabe em 1 batch
Resultado: Sucesso!
```

### Teste com 2507 Segmentos (RAND AIR)
```
‚ùå FALHOU (antes): 1800 > 136 limite real
‚úÖ VAI PASSAR (agora): 100 < 136 limite real
Resultado esperado: ~26 batches, ~6-7 minutos
```

## üéì Li√ß√£o Aprendida

**Dois limites diferentes**:

1. **Context Window** (200K tokens):
   - Quanto texto pode **RECEBER**
   - Afeta: Input do usu√°rio + System prompt + Hist√≥rico
   - N√ÉO afeta batch size!

2. **max_tokens** (8K tokens):
   - Quanto texto pode **GERAR**
   - Afeta: Output da resposta
   - **ESTE √© o limite real do batch size!**

**F√≥rmula Correta**:
```
batch_size_max = (max_tokens √ó fator_seguran√ßa) √∑ tokens_por_segmento_output
batch_size_max = (8192 √ó 0.85) √∑ 60
batch_size_max = ~116 segmentos
```

## üìä Novo Comportamento

### Log Esperado

```
================================================================================
üì¶ ESTRAT√âGIA DE TRADU√á√ÉO:
   Total de segmentos: 2507
   Segmentos por requisi√ß√£o: ~100
   N√∫mero de requisi√ß√µes: 26
   Modo: SEQUENCIAL (1 worker)
   üí° Cada requisi√ß√£o traduz ~100 segmentos de uma s√≥ vez!
================================================================================

  Traduzindo batch 1/26 (100 tokens)...
  üì§ Enviando 100 segmentos numa √öNICA requisi√ß√£o para Claude...
  üì• Resposta recebida do Claude para os 100 segmentos
  ‚úì Batch 1/26 completo

  Traduzindo batch 2/26 (100 tokens)...
  üì§ Enviando 100 segmentos numa √öNICA requisi√ß√£o para Claude...
  üì• Resposta recebida do Claude para os 100 segmentos
  ‚úì Batch 2/26 completo

  [... continua at√© batch 26 ...]
```

---

**Resumo**: O problema N√ÉO era o contexto (200K), era o `max_tokens` de output (8K). Batch sizes ajustados para **100-120 segmentos** ao inv√©s de 1800-2000! ‚úÖ
